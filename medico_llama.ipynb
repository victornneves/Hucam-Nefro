{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_dialog(patient_id):\n",
    "    PASS = \"first\"\n",
    "    with open(\n",
    "        f\"Dataset-Hucam-Nefro/patient_{patient_id:03d}/patient_{patient_id:03d}_transcription_{PASS}_pass.txt\",\n",
    "        \"r\",\n",
    "    ) as f:\n",
    "        prediction = f.read()\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def format_dict(data, indent=0):\n",
    "    \"\"\"\n",
    "    Recursively formats a dictionary or list into a readable string.\n",
    "    \"\"\"\n",
    "    formatted = []\n",
    "    prefix = \"  \" * indent  # Indentation for nesting\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            formatted.append(f\"{prefix}{key}:\")\n",
    "            formatted.append(format_dict(value, indent + 1))\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            formatted.append(f\"{prefix}- {format_dict(item, indent + 1).strip()}\")\n",
    "    else:\n",
    "        # For strings, numbers, or other types\n",
    "        formatted.append(f\"{prefix}{data}\")\n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "\n",
    "def readable_string(data):\n",
    "    if isinstance(data, dict):\n",
    "        # return json.dumps(data, indent=2, ensure_ascii=False) \n",
    "        return format_dict(data)\n",
    "    return data\n",
    "\n",
    "def get_diagnosis(hd):\n",
    "    try:\n",
    "        stage = hd[\"estagio_drc\"]\n",
    "        drc = \"\"\n",
    "        drc += f\"{stage['grau']}\"\n",
    "        if et := hd.get(\"etiologia_doença_de_base\"):\n",
    "            drc += \" \" + et\n",
    "        return drc\n",
    "    except TypeError:\n",
    "        return hd\n",
    "\n",
    "\n",
    "def get_field(summary, field):\n",
    "    field = summary[\"relatorio_consulta_ambulatorial_nefrologia\"][field]\n",
    "    if isinstance(field, list):\n",
    "        field = [readable_string(x) for x in field]\n",
    "        return \"\\n\".join(field)\n",
    "    else:\n",
    "        return field\n",
    "\n",
    "\n",
    "def get_summary(patient_id):\n",
    "    with open(\n",
    "        f\"Dataset-Hucam-Nefro/patient_{patient_id:03d}/patient_{patient_id:03d}_medical_summary.yaml\"\n",
    "    ) as fp:\n",
    "        return yaml.safe_load(fp)\n",
    "    \n",
    "def get_prediction(patient_id):\n",
    "    with open(\n",
    "        f\"results/patient_{patient_id}/outputs.yaml\"\n",
    "    ) as fp:\n",
    "        return yaml.safe_load(fp)\n",
    "\n",
    "def load_yaml(file_path):\n",
    "    \"\"\"Load a YAML file and return its content.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Define prompt template for the medical evaluator\n",
    "# prompt_template = (\n",
    "#     \"You are a medical evaluator tasked with comparing two texts related to medical diagnoses, clinical impressions, or proposed conduct. \"\n",
    "#     \"Your job is to assign a similarity score from 0 to 10, where 0 indicates complete discrepancy (e.g., antonyms or unrelated terms) \"\n",
    "#     \"and 10 indicates total similarity (e.g., synonymous terms or identical meanings). \"\n",
    "#     \"Consider the context, medical terminology, and overall meaning.\\n\\n\"\n",
    "#     \"Ground Truth: {ground_truth}\\n\"\n",
    "#     \"Prediction: {prediction}\\n\\n\"\n",
    "#     \"Score: \"\n",
    "# )\n",
    "\n",
    "# def evaluate_similarity_with_prompt(ground_truth, prediction):\n",
    "#     \"\"\"Evaluate similarity between ground truth and prediction using the model.\"\"\"\n",
    "#     prompt = prompt_template.format(ground_truth=ground_truth, prediction=prediction)\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "\n",
    "#     # Generate response\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model.generate(**inputs, max_length=256)\n",
    "#     response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "#     # Extract the score from the response\n",
    "#     try:\n",
    "#         score = float(response.split(\"Score:\")[-1].strip().split()[0])\n",
    "#     except (ValueError, IndexError):\n",
    "#         score = None\n",
    "\n",
    "#     return score\n",
    "\n",
    "# def evaluate_patient(patient_id):\n",
    "#     \"\"\"Evaluate a single patient's predictions against the ground truth.\"\"\"\n",
    "#     # Load YAML files\n",
    "#     ground_truth = get_summary(patient_id)\n",
    "#     prediction = get_prediction(patient_id)\n",
    "\n",
    "#     # Extract fields\n",
    "#     gt_diagnosis = get_diagnosis(ground_truth[\"relatorio_consulta_ambulatorial_nefrologia\"][\"hipoteses_diagnosticas\"])\n",
    "#     pred_diagnosis = get_diagnosis(prediction[\"hipoteses_diagnosticas\"])\n",
    "\n",
    "#     # gt_conduct = \"\\n\".join(readable_string(x) for x in get_field(ground_truth[\"relatorio_consulta_ambulatorial_nefrologia\"], \"conduta\"))\n",
    "#     # pred_conduct = \"\\n\".join(readable_string(x) for x in get_field(prediction, \"conduta\"))\n",
    "\n",
    "#     # gt_impression = \"\\n\".join(readable_string(x) for x in ground_truth[\"relatorio_consulta_ambulatorial_nefrologia\"].get(\"impressão\", []))\n",
    "#     # pred_impression = \"\\n\".join(readable_string(x) for x in prediction.get(\"impressão\", []))\n",
    "\n",
    "#     # Evaluate similarity\n",
    "#     diagnosis_score = evaluate_similarity_with_prompt(gt_diagnosis, pred_diagnosis)\n",
    "#     # conduct_score = evaluate_similarity_with_prompt(gt_conduct, pred_conduct)\n",
    "#     # impression_score = evaluate_similarity_with_prompt(gt_impression, pred_impression)\n",
    "\n",
    "#     # Compile results\n",
    "#     return {\n",
    "#         \"patient_id\": patient_id,\n",
    "#         \"GT Diagnosis\": gt_diagnosis,\n",
    "#         \"Pred Diagnosis\": pred_diagnosis,\n",
    "#         \"Diagnosis Similarity\": diagnosis_score,\n",
    "#         # \"GT Conduct\": gt_conduct,\n",
    "#         # \"Pred Conduct\": pred_conduct,\n",
    "#         # \"Conduct Similarity\": conduct_score,\n",
    "#         # \"GT Impression\": gt_impression,\n",
    "#         # \"Pred Impression\": pred_impression,\n",
    "#         # \"Impression Similarity\": impression_score\n",
    "#     }\n",
    "\n",
    "# # Iterate over patients and compile results\n",
    "# results = []\n",
    "# for patient_id in range(1, 17):\n",
    "#     print(f\"Evaluating patient {patient_id}\")\n",
    "#     patient_results = evaluate_patient(patient_id)\n",
    "#     if patient_results:\n",
    "#         results.append(patient_results)\n",
    "\n",
    "# # Save results to a DataFrame\n",
    "# results_df = pd.DataFrame(results)\n",
    "# results_df.to_csv(\"medical_similarity_results.csv\", index=False)\n",
    "# print(\"Evaluation complete. Results saved to 'medical_similarity_results.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "\n",
    "cache_dir = \"/mnt/external8tb/models\"\n",
    "model_id = \"aaditya/OpenBioLLM-Llama3-70B\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=cache_dir,\n",
    "    local_files_only=True,  # Force local loading\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "# Path to the resolved snapshot directory\n",
    "model_dir = \"/mnt/external8tb/models/models--aaditya--OpenBioLLM-Llama3-70B/snapshots/7ad17ef0d2185811f731f89d20885b2f99b1e994\"\n",
    "\n",
    "# Load the model and tokenizer explicitly\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    local_files_only=True,\n",
    "    device_map=\"auto\", \n",
    ")\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_dir,\n",
    "    local_files_only=True,\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Test the pipeline\n",
    "output = pipeline(\"Hello, world!\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your own chat prompt\n",
    "def create_chat_prompt(messages):\n",
    "    \"\"\"\n",
    "    Concatenate messages in a chat format for text generation.\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        role = message[\"role\"]\n",
    "        content = message[\"content\"]\n",
    "        if role == \"system\":\n",
    "            prompt += f\"System: {content}\\n\"\n",
    "        elif role == \"user\":\n",
    "            prompt += f\"User: {content}\\n\"\n",
    "        elif role == \"assistant\":\n",
    "            prompt += f\"Assistant: {content}\\n\"\n",
    "    prompt += \"Assistant: \"  # Add the generation prompt for the assistant\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# for patient_id in range(1, 17):\n",
    "for patient_id in [17]:\n",
    "\n",
    "    with open(f\"results/patient_{patient_id}/condensed_dialog.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        condensed_dialog = f.read()\n",
    "\n",
    "    summary = get_summary(patient_id)\n",
    "\n",
    "    # Get fields from the summary\n",
    "    fields = {\n",
    "        \"historico_patologico_pregresso\": get_field(summary, \"historico_patologico_pregresso\"),\n",
    "        \"exames_laboratoriais\": get_field(summary, \"exames_laboratoriais\"),\n",
    "        \"exames_complementares\": get_field(summary, \"exames_complementares\"),\n",
    "        \"exame_fisico\": get_field(summary, \"exame_fisico\"),\n",
    "        \"medicamentos_em_uso\": get_field(summary, \"medicamentos_em_uso\"),\n",
    "        \"identificacao_paciente\": get_field(summary, \"identificacao_paciente\"),\n",
    "    }\n",
    "\n",
    "    concatenated_info = (\n",
    "        f\"**Identificação do Paciente**\\n{fields['identificacao_paciente']}\\n\\n\"\n",
    "        f\"**Histórico Patológico Pregresso**\\n{fields['historico_patologico_pregresso']}\\n\\n\"\n",
    "        f\"**Exames Laboratoriais**\\n{fields['exames_laboratoriais']}\\n\\n\"\n",
    "        # f\"**Exames Complementares**\\n{fields['exames_complementares']}\\n\\n\"\n",
    "        # f\"**Exame Físico**\\n{fields['exame_fisico']}\\n\\n\"\n",
    "        # f\"**Medicamentos em Uso**\\n{fields['medicamentos_em_uso']}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    ground_truth = get_summary(patient_id)\n",
    "    prediction = get_prediction(patient_id)\n",
    "\n",
    "\n",
    "    # Extract fields\n",
    "    # GT = get_diagnosis(ground_truth[\"relatorio_consulta_ambulatorial_nefrologia\"][\"hipoteses_diagnosticas\"])\n",
    "    # PRED = get_diagnosis(prediction[\"hipoteses_diagnosticas\"])\n",
    "    # GT = get_field(ground_truth, \"impressão\")\n",
    "    # PRED = \"\\n\".join(prediction[\"impressão\"])\n",
    "    GT = get_field(ground_truth, \"conduta\")\n",
    "    PRED = \"\\n\".join(prediction[\"conduta\"])\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Você é um especialista em nefrologia com ampla experiência em avaliação de casos clínicos e análises médicas. \"\n",
    "                \"Você você irá fornecer avaliações detalhadas e contextualizadas de informações clínicas. \"\n",
    "                # \"Seu objetivo é revisar uma consulta de paciente com um médico nefrologista, e analisar duas opiniões médicas distintas sobre a etiologia da doença renal crônica do paciente, \"\n",
    "                \"Seu objetivo é revisar o relatório de uma consulta paciente, com base nos dados fornecidos e em duas opiniões médicas distintas sobre como deve ser a conduta ao paciente, \"\n",
    "                # \"Seu objetivo é revisar uma consulta de paciente com um médico nefrologista, e analisar duas opiniões médicas distintas acerca da impressão clínica, \"\n",
    "                \"e fornecer uma análise sobre a similaridade e precisão dessas duas opiniões. \"\n",
    "                \"Use seu conhecimento em nefrologia, diretrizes clínicas e critérios diagnósticos para avaliar a validade e a coerência das informações fornecidas.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Você receberá um sumário da conversa de uma consulta ambulatorial de um paciente com um médico nefrologia. \"\n",
    "                \"Além disso, você receberá as informações completas do paciente, incluindo histórico, exames laboratoriais.\"\n",
    "                # \"Serão fornecidas duas opiniões médicas distintas sobre a etiologia da doença renal crônica do paciente, \"\n",
    "                \"Serão fornecidas duas opiniões médicas distintas sobre a conduta ao paciente\"\n",
    "                # \"Serão fornecidas duas opiniões médicas distintas sobre a impressão clínica\"\n",
    "                \"e sua tarefa será avaliar o seguinte:\\n\\n\"\n",
    "                \"1. Analisar se as duas opiniões são suficientement semelhantes ou complemtamente diferentes.\\n\"\n",
    "                \"2. Avaliar se cada uma delas é plausível, dado o contexto clínico do paciente e a conversa da consulta.\\n\"\n",
    "                \"3. Atribuir um dos seguintes scores com base em sua análise:\\n\"\n",
    "                \"- Different_Both_Correct: As opiniões são muito diferentes, mas ambas são plausíveis.\\n\"\n",
    "                \"- Different_first_Correct: As opiniões são muito diferentes, e apenas a primeira é plausível.\\n\"\n",
    "                \"- Different_second_Correct: As opiniões são muito diferentes, e apenas a segunda é plausível.\\n\"\n",
    "                \"- Different_Both_Incorrect: As opiniões são muito diferentes, e nenhuma é plausível.\\n\\n\"\n",
    "                \"- Aligned_Correct: As opiniões são suficientemente semelhantes e ambas são plausíveis.\\n\"\n",
    "                \"- Aligned_Incorrect: As opiniões são suficientemente semelhantes, e nenhuma é plausível.\\n\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"**Sumário da consulta ambulatorial**\\n{condensed_dialog}\\n\\n\"\n",
    "                f\"**Informações do Paciente**\\n{concatenated_info}\\n\\n\"\n",
    "                \"**Opinião Médica 1**:\\n\"\n",
    "                f\"{GT}\\n\\n\"\n",
    "                \"**Opinião Médica 2**:\\n\"\n",
    "                f\"{PRED}\\n\\n\"\n",
    "                \"Com base nas informações acima, avalie e forneça seu score.\"\n",
    "                \"Justifique sua resposta.\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Generate the prompt manually\n",
    "    prompt = create_chat_prompt(messages)\n",
    "\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=pipeline.tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    # Print the generated output\n",
    "    print(\"GT\", GT)\n",
    "    print(\"PRED\", PRED)\n",
    "    print(patient_id, outputs[0][\"generated_text\"][len(prompt):])\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
