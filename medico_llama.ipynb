{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_dialog(patient_id):\n",
    "    PASS = \"first\"\n",
    "    with open(\n",
    "        f\"/mnt/external8tb/datasets/Dataset-Hucam-Nefro/patient_{patient_id:03d}/patient_{patient_id:03d}_transcription_{PASS}_pass.txt\",\n",
    "        \"r\",\n",
    "    ) as f:\n",
    "        prediction = f.read()\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def format_dict(data, indent=0):\n",
    "    \"\"\"\n",
    "    Recursively formats a dictionary or list into a readable string.\n",
    "    \"\"\"\n",
    "    formatted = []\n",
    "    prefix = \"  \" * indent  # Indentation for nesting\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, (dict, list)):\n",
    "                formatted.append(f\"{prefix}{key}:\")\n",
    "                formatted.append(format_dict(value, indent + 1))\n",
    "            else:\n",
    "                formatted.append(f\"{prefix}{key}: {value}\")\n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "def get_diagnosis(hd):\n",
    "    try:\n",
    "        stage = hd[\"estagio_drc\"]\n",
    "        drc = \"\"\n",
    "        drc += f\"{stage['grau']}\"\n",
    "        if et := hd.get(\"etiologia_doença_de_base\"):\n",
    "            drc += \" \" + et\n",
    "        return drc\n",
    "    except TypeError:\n",
    "        return hd\n",
    "\n",
    "\n",
    "def readable_string(x):\n",
    "    \"\"\"\n",
    "    Converts a dictionary or list to a readable string using YAML formatting.\n",
    "    \"\"\"\n",
    "    if isinstance(x, (dict, list)):\n",
    "        return yaml.dump(x, allow_unicode=True, default_flow_style=False).strip()\n",
    "    return str(x)\n",
    "\n",
    "def get_field(summary, field):\n",
    "    field = summary[\"relatorio_consulta_ambulatorial_nefrologia\"][field]\n",
    "    if isinstance(field, list):\n",
    "        field = [readable_string(x) for x in field]\n",
    "        return \"\\n\".join(field)\n",
    "    else:\n",
    "        return field\n",
    "\n",
    "\n",
    "def get_summary(patient_id):\n",
    "    with open(\n",
    "        f\"/mnt/external8tb/datasets/Dataset-Hucam-Nefro/patient_{patient_id:03d}/patient_{patient_id:03d}_medical_summary.yaml\"\n",
    "    ) as fp:\n",
    "        return yaml.safe_load(fp)\n",
    "    \n",
    "def get_prediction(patient_id):\n",
    "    with open(\n",
    "        f\"results/patient_{patient_id}/outputs.yaml\"\n",
    "    ) as fp:\n",
    "        return yaml.safe_load(fp)\n",
    "\n",
    "def load_yaml(file_path):\n",
    "    \"\"\"Load a YAML file and return its content.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "\n",
    "cache_dir = \"/mnt/external8tb/models\"\n",
    "model_id = \"aaditya/OpenBioLLM-Llama3-70B\"\n",
    "\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\n",
    "        \"quantization_config\": quantization_config,\n",
    "        \"cache_dir\": cache_dir,\n",
    "        \"local_files_only\": True,\n",
    "    },\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# import torch\n",
    "\n",
    "# # Path to the resolved snapshot directorypipeline\n",
    "# model_dir = \"/mnt/external8tb/models/models--aaditya--OpenBioLLM-Llama3-70B/snapshots/7ad17ef0d2185811f731f89d20885b2f99b1e994\"\n",
    "\n",
    "# # Load the model and tokenizer explicitly\n",
    "# model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "#     model_dir,\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     local_files_only=True,\n",
    "#     device_map=\"auto\", \n",
    "# )\n",
    "\n",
    "# tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "#     model_dir,\n",
    "#     local_files_only=True,\n",
    "# )\n",
    "\n",
    "# # Create the pipeline\n",
    "# pipeline = transformers.pipeline(\n",
    "#     \"text-generation\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     # device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# # Test the pipeline\n",
    "# output = pipeline(\"Hello, world!\")\n",
    "# print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your own chat prompt\n",
    "def create_chat_prompt(messages):\n",
    "    \"\"\"\n",
    "    Concatenate messages in a chat format for text generation.\n",
    "    \"\"\"\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        role = message[\"role\"]\n",
    "        content = message[\"content\"]\n",
    "        if role == \"system\":\n",
    "            prompt += f\"System: {content}\\n\"\n",
    "        elif role == \"user\":\n",
    "            prompt += f\"User: {content}\\n\"\n",
    "        elif role == \"assistant\":\n",
    "            prompt += f\"Assistant: {content}\\n\"\n",
    "    prompt += \"Assistant: \"  # Add the generation prompt for the assistant\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "for patient_id in range(18, 27):\n",
    "\n",
    "    with open(f\"results/patient_{patient_id}/condensed_dialog.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        condensed_dialog = f.read()\n",
    "\n",
    "    summary = get_summary(patient_id)\n",
    "\n",
    "    # Get fields from the summary\n",
    "    fields = {\n",
    "        \"historico_patologico_pregresso\": get_field(summary, \"historico_patologico_pregresso\"),\n",
    "        \"exames_laboratoriais\": get_field(summary, \"exames_laboratoriais\"),\n",
    "        \"exames_complementares\": get_field(summary, \"exames_complementares\"),\n",
    "        \"exame_fisico\": get_field(summary, \"exame_fisico\"),\n",
    "        \"medicamentos_em_uso\": get_field(summary, \"medicamentos_em_uso\"),\n",
    "        \"identificacao_paciente\": get_field(summary, \"identificacao_paciente\"),\n",
    "    }\n",
    "\n",
    "    concatenated_info = (\n",
    "        f\"**Identificação do Paciente**\\n{fields['identificacao_paciente']}\\n\\n\"\n",
    "        f\"**Histórico Patológico Pregresso**\\n{fields['historico_patologico_pregresso']}\\n\\n\"\n",
    "        f\"**Exames Laboratoriais**\\n{fields['exames_laboratoriais']}\\n\\n\"\n",
    "        # f\"**Exames Complementares**\\n{fields['exames_complementares']}\\n\\n\"\n",
    "        # f\"**Exame Físico**\\n{fields['exame_fisico']}\\n\\n\"\n",
    "        # f\"**Medicamentos em Uso**\\n{fields['medicamentos_em_uso']}\\n\\n\"\n",
    "    )\n",
    "\n",
    "    ground_truth = get_summary(patient_id)\n",
    "    prediction = get_prediction(patient_id)\n",
    "\n",
    "\n",
    "    # Extract fields\n",
    "    # GT = get_diagnosis(ground_truth[\"relatorio_consulta_ambulatorial_nefrologia\"][\"hipoteses_diagnosticas\"])\n",
    "    # PRED = get_diagnosis(prediction[\"hipoteses_diagnosticas\"])\n",
    "    # GT = get_field(ground_truth, \"impressão\")\n",
    "    # PRED = \"\\n\".join(prediction[\"impressão\"])\n",
    "    GT = get_field(ground_truth, \"conduta\")\n",
    "    PRED = \"\\n\".join(prediction[\"conduta\"])\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"Você é um especialista em nefrologia com ampla experiência em avaliação de casos clínicos e análises médicas. \"\n",
    "                \"Você você irá fornecer avaliações detalhadas e contextualizadas de informações clínicas. \"\n",
    "                # \"Seu objetivo é revisar uma consulta de paciente com um médico nefrologista, e analisar duas opiniões médicas distintas sobre a etiologia da doença renal crônica do paciente, \"\n",
    "                # \"Seu objetivo é revisar o relatório de uma consulta paciente, com base nos dados fornecidos e em duas opiniões médicas distintas sobre como deve ser a conduta ao paciente, \"\n",
    "                \"Seu objetivo é revisar uma consulta de paciente com um médico nefrologista, e analisar duas opiniões médicas distintas acerca da impressão clínica, \"\n",
    "                \"e fornecer uma análise sobre a similaridade e precisão dessas duas opiniões. \"\n",
    "                \"Use seu conhecimento em nefrologia, diretrizes clínicas e critérios diagnósticos para avaliar a validade e a coerência das informações fornecidas.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Você receberá um sumário da conversa de uma consulta ambulatorial de um paciente com um médico nefrologia. \"\n",
    "                \"Além disso, você receberá as informações completas do paciente, incluindo histórico, exames laboratoriais.\"\n",
    "                # \"Serão fornecidas duas opiniões médicas distintas sobre a etiologia da doença renal crônica do paciente, \"\n",
    "                # \"Serão fornecidas duas opiniões médicas distintas sobre a conduta ao paciente\"\n",
    "                \"Serão fornecidas duas opiniões médicas distintas sobre a impressão clínica\"\n",
    "                \"e sua tarefa será avaliar o seguinte:\\n\\n\"\n",
    "                \"1. Analisar se as duas opiniões são suficientement semelhantes ou complemtamente diferentes.\\n\"\n",
    "                \"2. Avaliar se cada uma delas é plausível, dado o contexto clínico do paciente e a conversa da consulta.\\n\"\n",
    "                \"3. Atribuir um dos seguintes scores com base em sua análise:\\n\"\n",
    "                \"- Different_Both_Correct: As opiniões são muito diferentes, mas ambas são plausíveis.\\n\"\n",
    "                \"- Different_first_Correct: As opiniões são muito diferentes, e apenas a primeira é plausível.\\n\"\n",
    "                \"- Different_second_Correct: As opiniões são muito diferentes, e apenas a segunda é plausível.\\n\"\n",
    "                \"- Different_Both_Incorrect: As opiniões são muito diferentes, e nenhuma é plausível.\\n\\n\"\n",
    "                \"- Aligned_Correct: As opiniões são suficientemente semelhantes e ambas são plausíveis.\\n\"\n",
    "                \"- Aligned_Incorrect: As opiniões são suficientemente semelhantes, e nenhuma é plausível.\\n\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"**Sumário da consulta ambulatorial**\\n{condensed_dialog}\\n\\n\"\n",
    "                f\"**Informações do Paciente**\\n{concatenated_info}\\n\\n\"\n",
    "                \"**Opinião Médica 1**:\\n\"\n",
    "                f\"{GT}\\n\\n\"\n",
    "                \"**Opinião Médica 2**:\\n\"\n",
    "                f\"{PRED}\\n\\n\"\n",
    "                \"Com base nas informações acima, avalie e forneça seu score.\"\n",
    "                \"Justifique sua resposta.\"\n",
    "            )\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Generate the prompt manually!pip uninstall bitsandbytes -y\n",
    "    prompt = create_chat_prompt(messages)\n",
    "\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=pipeline.tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=1.0,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "    # Print the generated output\n",
    "    print(\"GT\", GT)\n",
    "    print(\"PRED\", PRED)\n",
    "    print(patient_id, outputs[0][\"generated_text\"][len(prompt):])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
